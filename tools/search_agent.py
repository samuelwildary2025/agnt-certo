"""Ferramenta de Sub-Agente para Busca Especializada de Produtos"""
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from pydantic.v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

from config.settings import settings
from config.logger import setup_logger
from tools.vector_search_subagent import run_vector_search_subagent
from tools.http_tools import estoque_preco

logger = setup_logger(__name__)

_ANALISTA_PROMPT_CACHE: Optional[str] = None



def _load_analista_prompt() -> str:
    global _ANALISTA_PROMPT_CACHE
    if _ANALISTA_PROMPT_CACHE is not None:
        return _ANALISTA_PROMPT_CACHE

    base_dir = Path(__file__).resolve().parent.parent
    prompt_path = base_dir / "prompts" / "analista.md"
    _ANALISTA_PROMPT_CACHE = prompt_path.read_text(encoding="utf-8")
    return _ANALISTA_PROMPT_CACHE





@tool("banco_vetorial")
def banco_vetorial_tool(query: str, limit: int = 10) -> str:
    """
    Realiza uma busca vetorial no banco de dados de produtos.
    Retorna uma lista de produtos mais similares semanticamente √† query.
    """
    return run_vector_search_subagent(query, limit=limit)


@tool("estoque_preco")
def estoque_preco_tool(ean: str) -> str:
    """
    Consulta o estoque e pre√ßo atual de um produto pelo seu c√≥digo EAN.
    Retorna JSON com dados atualizados.
    """
    return estoque_preco(ean)


@tool("calculadora")
def calculadora_tool(expressao: str) -> str:
    """
    Calculadora simples. Avalia express√µes matem√°ticas b√°sicas.
    Use para calcular quantidade = valor / preco_kg.
    Ex: calculadora("5 / 40") retorna "0.125"
    """
    try:
        # Sanitizar express√£o (apenas permitir n√∫meros e operadores b√°sicos)
        allowed_chars = set("0123456789.+-*/() ")
        if not all(c in allowed_chars for c in expressao):
            return "Erro: Express√£o inv√°lida"
        result = eval(expressao)
        return str(round(result, 3))
    except Exception as e:
        return f"Erro: {e}"


def _run_analista_agent_for_term(term: str, telefone: Optional[str] = None) -> dict:
    prompt = _load_analista_prompt()
    
    llm = _get_fast_llm()
    agent = create_react_agent(llm, [banco_vetorial_tool, estoque_preco_tool], prompt=prompt)

    user_payload = json.dumps(
        {"termo": term},
        ensure_ascii=False,
    )

    config = {"recursion_limit": 8}
    if telefone:
        config["configurable"] = {"thread_id": telefone}

    result = agent.invoke({"messages": [HumanMessage(content=user_payload)]}, config)
    messages = result.get("messages", []) if isinstance(result, dict) else []

    for m in reversed(messages):
        if getattr(m, "type", None) != "ai":
            continue
        content = m.content if isinstance(m.content, str) else str(m.content)
        content = (content or "").strip()
        if not content:
            continue
        try:
            return json.loads(content)
        except Exception:
            return {"ok": False, "termo": term, "motivo": "Resposta nao-JSON do analista"}


    return {"ok": False, "termo": term, "motivo": "Sem resposta"}


# TERM_EXTRACTOR_PROMPT REMOVIDO - Simplifica√ß√£o do fluxo
# O Vendedor envia os termos j√° separados e o Analista resolve

# ============================================
# 2. Configura√ß√µes do Modelo
# ============================================

_HTTP_CLIENT_CACHE = None
_HTTP_ASYNC_CLIENT_CACHE = None

def _openai_model_supports_temperature(model: str) -> bool:
    m = (model or "").lower().strip()
    if m.startswith("gpt-5") or m.startswith("gpt5") or "gpt-5" in m:
        return False
    return True

def _get_fast_llm():
    """Retorna um modelo r√°pido e barato para tarefas de sub-agente."""
    global _HTTP_CLIENT_CACHE, _HTTP_ASYNC_CLIENT_CACHE

    # PREFER√äNCIA: Usar o modelo configurado no settings (ex: grok-beta)
    model_name = getattr(settings, "llm_model", "gemini-2.5-flash")
    temp = float(getattr(settings, "llm_temperature", 0.0))

    if settings.llm_provider == "google":
        return ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=settings.google_api_key,
            temperature=temp
        )
    else:
        client_kwargs = {}
        if settings.openai_api_base:
            client_kwargs["base_url"] = settings.openai_api_base

        import httpx
        
        # Singleton Clients para evitar abrir mil conex√µes no loop
        if _HTTP_CLIENT_CACHE is None:
            _HTTP_CLIENT_CACHE = httpx.Client(timeout=30.0)
        if _HTTP_ASYNC_CLIENT_CACHE is None:
            _HTTP_ASYNC_CLIENT_CACHE = httpx.AsyncClient(timeout=30.0)
        
        if _openai_model_supports_temperature(model_name):
            return ChatOpenAI(
                model=model_name,
                api_key=settings.openai_api_key,
                temperature=temp,
                http_client=_HTTP_CLIENT_CACHE,
                http_async_client=_HTTP_ASYNC_CLIENT_CACHE,
                **client_kwargs
            )
        return ChatOpenAI(
            model=model_name,
            api_key=settings.openai_api_key,
            http_client=_HTTP_CLIENT_CACHE,
            http_async_client=_HTTP_ASYNC_CLIENT_CACHE,
            **client_kwargs
        )

# ============================================
# 3. Fun√ß√£o Principal (Tool)
# ============================================

def analista_produtos_tool(queries_str: str, telefone: str = None) -> str:
    """
    [ANALISTA DE PRODUTOS]
    Agente Especialista que traduz pedidos do cliente em produtos reais do banco de dados.
    Usa busca vetorial + intelig√™ncia sem√¢ntica.
    
    Args:
        queries_str: Termos de busca (ex: "arroz, feij√£o, p√£o").
        telefone: Opcional - n√∫mero do cliente para salvar sugest√µes no cache.
    """
    results = []
    validated_products = []  # Para cache no Redis
    
    # SIMPLIFICADO: Separa√ß√£o simples por v√≠rgula ou newline (sem LLM intermedi√°rio)
    # O Vendedor j√° envia termos limpos e o Analista resolve o significado
    extracted_terms = [t.strip() for t in queries_str.replace("\n", ",").split(",") if t.strip()]

    mode = "lote" if len(extracted_terms) > 1 else "individual"
    logger.info(f"üïµÔ∏è [SUB-AGENT] Modo de busca: {mode} | termos: {extracted_terms}")
    
    # Fun√ß√£o helper para processar cada termo em paralelo
    def _process_single_term(term: str):
        try:
            decision = _run_analista_agent_for_term(term, telefone=telefone)
            if not isinstance(decision, dict) or not decision.get("ok"):
                motivo = (decision or {}).get("motivo") if isinstance(decision, dict) else None
                return (f"‚ùå {term}: {motivo or 'Nao encontrado'}", None)

            # MODO MULTIPLAS OP√á√ïES
            opcoes = decision.get("opcoes")
            if opcoes and isinstance(opcoes, list) and len(opcoes) > 0:
                out_lines = [f"üìã [ANALISTA] OP√á√ïES PARA '{term}' (Pergunte ao cliente):"]
                for i, opt in enumerate(opcoes, 1):
                    n = opt.get("nome", "Item")
                    p = float(opt.get("preco", 0.0))
                    out_lines.append(f"   {i}. {n} - R$ {p:.2f}")
                
                out_lines.append("\n‚ö†Ô∏è N√ÉO Adicionado automaticamente. Liste as op√ß√µes para o cliente.")
                return ("\n".join(out_lines), None)

            # MODO √öNICO
            nome = str(decision.get("nome") or "").strip()
            preco = float(decision.get("preco") or 0.0)

            if not nome:
                return (f"‚ùå {term}: Resposta incompleta do analista", None)

            validated_item = {"nome": nome, "preco": preco, "termo_busca": term}
            razao = str(decision.get("razao") or "").strip()
            
            result_str = (
                "üîç [ANALISTA] ITEM VALIDADO:\n"
                f"- Nome: {nome}\n"
                f"- Pre√ßo Tabela: R$ {preco:.2f}\n"
                f"- Obs: {razao}\n"
                f"\nüîî DICA: Item encontrado com sucesso.\n"
                f"- Se o cliente pediu para COMPRAR/ADICIONAR: use add_item_tool.\n"
                f"- Se o cliente apenas PERGUNTOU PRE√áO/TEM: responda apenas com o pre√ßo."
            )
            return (result_str, validated_item)
            
        except Exception as e:
            logger.error(f"‚ùå [SUB-AGENT] Erro no agente Analista para '{term}': {e}")
            return (f"‚ùå {term}: Erro interno na busca.", None)

    # Execu√ß√£o Paralela
    import concurrent.futures
    
    # Limitar n√∫mero de workers para n√£o saturar
    max_workers = min(10, len(extracted_terms) + 1)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submeter tarefas mantendo a ordem: mapa {future: index}
        future_to_index = {
            executor.submit(_process_single_term, term): i 
            for i, term in enumerate(extracted_terms)
        }
        
        # Array para guardar resultados na ordem correta
        ordered_results = [None] * len(extracted_terms)
        
        for future in concurrent.futures.as_completed(future_to_index):
            index = future_to_index[future]
            try:
                ordered_results[index] = future.result()
            except Exception as e:
                logger.error(f"Erro fatal processando future index {index}: {e}")
                ordered_results[index] = (f"‚ùå Erro interno.", None)
                
    # Coletar resultados finais
    for res in ordered_results:
        if not res: 
            continue
        res_str, val_item = res
        if res_str:
            results.append(res_str)
        if val_item:
            validated_products.append(val_item)

    # SALVAR CACHE NO REDIS SE TIVER TELEFONE
    if telefone and validated_products:
        try:
            from tools.redis_tools import save_suggestions
            save_suggestions(telefone, validated_products)
            logger.info(f"üíæ [SUB-AGENT] Cache salvo: {len(validated_products)} produtos para {telefone}")
        except Exception as e:
            logger.error(f"Erro ao salvar cache de sugest√µes: {e}")

    if not results:
        return "Nenhum produto encontrado."
        
    return "\n".join(results)
